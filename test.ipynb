{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"title\": \"Olympus: A Universal Task Router for Computer Vision Tasks\",\n",
      "  \"link\": \"https://papers.cool/arxiv/2412.09612\",\n",
      "  \"description\": \"<a href=\\\"https://arxiv.org/pdf/2412.09612.pdf\\\">[PDF]</a>   <a href=\\\"https://papers.cool/arxiv/2412.09612\\\">[Site]</a>   <a href=\\\"https://papers.cool/arxiv/2412.09612\\\">[Kimi]</a>   <p><b>Authors:</b> Yuanze Lin</p>   <p>We introduce Olympus, a new approach that transforms Multimodal Large Language Models (MLLMs) into a unified framework capable of handling a wide array of computer vision tasks. Utilizing a controller MLLM, Olympus delegates over 20 specialized tasks across images, videos, and 3D objects to dedicated modules. This instruction-based routing enables complex workflows through chained actions without the need for training heavy generative models. Olympus easily integrates with existing MLLMs, expanding their capabilities with comparable performance. Experimental results demonstrate that Olympus achieves an average routing accuracy of 94.75% across 20 tasks and precision of 91.82% in chained action scenarios, showcasing its effectiveness as a universal task router that can solve a diverse range of computer vision tasks. Project page: https://github.com/yuanze-lin/Olympus_page</p>\"\n",
      "}\n",
      "{\n",
      "  \"title\": \"TimeRefine: Temporal Grounding with Time Refining Video LLM\",\n",
      "  \"link\": \"https://papers.cool/arxiv/2412.09601\",\n",
      "  \"description\": \"<a href=\\\"https://arxiv.org/pdf/2412.09601.pdf\\\">[PDF]</a>   <a href=\\\"https://papers.cool/arxiv/2412.09601\\\">[Site]</a>   <a href=\\\"https://papers.cool/arxiv/2412.09601\\\">[Kimi]</a>   <p><b>Authors:</b> Xizi Wang</p>   <p>Video temporal grounding aims to localize relevant temporal boundaries in a video given a textual prompt. Recent work has focused on enabling Video LLMs to perform video temporal grounding via next-token prediction of temporal timestamps. However, accurately localizing timestamps in videos remains challenging for Video LLMs when relying solely on temporal token prediction. Our proposed TimeRefine addresses this challenge in two ways. First, instead of directly predicting the start and end timestamps, we reformulate the temporal grounding task as a temporal refining task: the model first makes rough predictions and then refines them by predicting offsets to the target segment. This refining process is repeated multiple times, through which the model progressively self-improves its temporal localization accuracy. Second, to enhance the model's temporal perception capabilities, we incorporate an auxiliary prediction head that penalizes the model more if a predicted segment deviates further from the ground truth, thus encouraging the model to make closer and more accurate predictions. Our plug-and-play method can be integrated into most LLM-based temporal grounding approaches. The experimental results demonstrate that TimeRefine achieves 3.6% and 5.0% mIoU improvements on the ActivityNet and Charades-STA datasets, respectively. Code and pretrained models will be released.</p>\"\n",
      "}\n",
      "{\n",
      "  \"title\": \"InternLM-XComposer2.5-OmniLive: A Comprehensive Multimodal System for Long-term Streaming Video and Audio Interactions\",\n",
      "  \"link\": \"https://papers.cool/arxiv/2412.09596\",\n",
      "  \"description\": \"<a href=\\\"https://arxiv.org/pdf/2412.09596.pdf\\\">[PDF]</a>   <a href=\\\"https://papers.cool/arxiv/2412.09596\\\">[Site]</a>   <a href=\\\"https://papers.cool/arxiv/2412.09596\\\">[Kimi]</a>   <p><b>Authors:</b> Pan Zhang</p>   <p>Creating AI systems that can interact with environments over long periods, similar to human cognition, has been a longstanding research goal. Recent advancements in multimodal large language models (MLLMs) have made significant strides in open-world understanding. However, the challenge of continuous and simultaneous streaming perception, memory, and reasoning remains largely unexplored. Current MLLMs are constrained by their sequence-to-sequence architecture, which limits their ability to process inputs and generate responses simultaneously, akin to being unable to think while perceiving. Furthermore, relying on long contexts to store historical data is impractical for long-term interactions, as retaining all information becomes costly and inefficient. Therefore, rather than relying on a single foundation model to perform all functions, this project draws inspiration from the concept of the Specialized Generalist AI and introduces disentangled streaming perception, reasoning, and memory mechanisms, enabling real-time interaction with streaming video and audio input. The proposed framework InternLM-XComposer2.5-OmniLive (IXC2.5-OL) consists of three key modules: (1) Streaming Perception Module: Processes multimodal information in real-time, storing key details in memory and triggering reasoning in response to user queries. (2) Multi-modal Long Memory Module: Integrates short-term and long-term memory, compressing short-term memories into long-term ones for efficient retrieval and improved accuracy. (3) Reasoning Module: Responds to queries and executes reasoning tasks, coordinating with the perception and memory modules. This project simulates human-like cognition, enabling multimodal large language models to provide continuous and adaptive service over time.</p>\"\n",
      "}\n",
      "{\n",
      "  \"title\": \"Neptune: The Long Orbit to Benchmarking Long Video Understanding\",\n",
      "  \"link\": \"https://papers.cool/arxiv/2412.09582\",\n",
      "  \"description\": \"<a href=\\\"https://arxiv.org/pdf/2412.09582.pdf\\\">[PDF]</a>   <a href=\\\"https://papers.cool/arxiv/2412.09582\\\">[Site]</a>   <a href=\\\"https://papers.cool/arxiv/2412.09582\\\">[Kimi]</a>   <p><b>Authors:</b> Arsha Nagrani</p>   <p>This paper describes a semi-automatic pipeline to generate challenging question-answer-decoy sets for understanding long videos. Many existing video datasets and models are focused on short clips (10s-30s). While some long video datasets do exist, they can often be solved by powerful image models applied per frame (and often to very few frames) in a video, and are usually manually annotated at high cost. In order to mitigate both these problems, we propose a scalable dataset creation pipeline which leverages large models (VLMs and LLMs), to automatically generate dense, time-aligned video captions, as well as tough question answer decoy sets for video segments (up to 15 minutes in length). Our dataset Neptune covers a broad range of long video reasoning abilities and consists of a subset that emphasizes multimodal reasoning. Since existing metrics for open-ended question answering are either rule-based or may rely on proprietary models, we provide a new open source model-based metric GEM to score open-ended responses on Neptune. Benchmark evaluations reveal that most current open-source long video models perform poorly on Neptune, particularly on questions testing temporal ordering, counting and state changes. Through Neptune, we aim to spur the development of more advanced models capable of understanding long videos. The dataset is available at https://github.com/google-deepmind/neptune</p>\"\n",
      "}\n",
      "{\n",
      "  \"title\": \"JuStRank: Benchmarking LLM Judges for System Ranking\",\n",
      "  \"link\": \"https://papers.cool/arxiv/2412.09569\",\n",
      "  \"description\": \"<a href=\\\"https://arxiv.org/pdf/2412.09569.pdf\\\">[PDF]</a>   <a href=\\\"https://papers.cool/arxiv/2412.09569\\\">[Site]</a>   <a href=\\\"https://papers.cool/arxiv/2412.09569\\\">[Kimi]</a>   <p><b>Authors:</b> Ariel Gera</p>   <p>Given the rapid progress of generative AI, there is a pressing need to systematically compare and choose between the numerous models and configurations available. The scale and versatility of such evaluations make the use of LLM-based judges a compelling solution for this challenge. Crucially, this approach requires first to validate the quality of the LLM judge itself. Previous work has focused on instance-based assessment of LLM judges, where a judge is evaluated over a set of responses, or response pairs, while being agnostic to their source systems. We argue that this setting overlooks critical factors affecting system-level ranking, such as a judge's positive or negative bias towards certain systems. To address this gap, we conduct the first large-scale study of LLM judges as system rankers. System scores are generated by aggregating judgment scores over multiple system outputs, and the judge's quality is assessed by comparing the resulting system ranking to a human-based ranking. Beyond overall judge assessment, our analysis provides a fine-grained characterization of judge behavior, including their decisiveness and bias.</p>\"\n",
      "}\n",
      "{\n",
      "  \"title\": \"From Intention To Implementation: Automating Biomedical Research via LLMs\",\n",
      "  \"link\": \"https://papers.cool/arxiv/2412.09429\",\n",
      "  \"description\": \"<a href=\\\"https://arxiv.org/pdf/2412.09429.pdf\\\">[PDF]</a>   <a href=\\\"https://papers.cool/arxiv/2412.09429\\\">[Site]</a>   <a href=\\\"https://papers.cool/arxiv/2412.09429\\\">[Kimi]</a>   <p><b>Authors:</b> Yi Luo</p>   <p>Conventional biomedical research is increasingly labor-intensive due to the exponential growth of scientific literature and datasets. Artificial intelligence (AI), particularly Large Language Models (LLMs), has the potential to revolutionize this process by automating various steps. Still, significant challenges remain, including the need for multidisciplinary expertise, logicality of experimental design, and performance measurements. This paper introduces BioResearcher, the first end-to-end automated system designed to streamline the entire biomedical research process involving dry lab experiments. BioResearcher employs a modular multi-agent architecture, integrating specialized agents for search, literature processing, experimental design, and programming. By decomposing complex tasks into logically related sub-tasks and utilizing a hierarchical learning approach, BioResearcher effectively addresses the challenges of multidisciplinary requirements and logical complexity. Furthermore, BioResearcher incorporates an LLM-based reviewer for in-process quality control and introduces novel evaluation metrics to assess the quality and automation of experimental protocols. BioResearcher successfully achieves an average execution success rate of 63.07% across eight previously unmet research objectives. The generated protocols averagely outperform typical agent systems by 22.0% on five quality metrics. The system demonstrates significant potential to reduce researchers' workloads and accelerate biomedical discoveries, paving the way for future innovations in automated research systems.</p>\"\n",
      "}\n",
      "{\n",
      "  \"title\": \"AI Predicts AGI: Leveraging AGI Forecasting and Peer Review to Explore LLMs' Complex Reasoning Capabilities\",\n",
      "  \"link\": \"https://papers.cool/arxiv/2412.09385\",\n",
      "  \"description\": \"<a href=\\\"https://arxiv.org/pdf/2412.09385.pdf\\\">[PDF]</a>   <a href=\\\"https://papers.cool/arxiv/2412.09385\\\">[Site]</a>   <a href=\\\"https://papers.cool/arxiv/2412.09385\\\">[Kimi]</a>   <p><b>Authors:</b> Fabrizio Davide</p>   <p>We tasked 16 state-of-the-art large language models (LLMs) with estimating the likelihood of Artificial General Intelligence (AGI) emerging by 2030. To assess the quality of these forecasts, we implemented an automated peer review process (LLM-PR). The LLMs' estimates varied widely, ranging from 3% (Reka- Core) to 47.6% (GPT-4o), with a median of 12.5%. These estimates closely align with a recent expert survey that projected a 10% likelihood of AGI by 2027, underscoring the relevance of LLMs in forecasting complex, speculative scenarios. The LLM-PR process demonstrated strong reliability, evidenced by a high Intraclass Correlation Coefficient (ICC = 0.79), reflecting notable consistency in scoring across the models. Among the models, Pplx-70b-online emerged as the top performer, while Gemini-1.5-pro-api ranked the lowest. A cross-comparison with external benchmarks, such as LMSYS Chatbot Arena, revealed that LLM rankings remained consistent across different evaluation methods, suggesting that existing benchmarks may not encapsulate some of the skills relevant for AGI prediction. We further explored the use of weighting schemes based on external benchmarks, optimizing the alignment of LLMs' predictions with human expert forecasts. This analysis led to the development of a new, 'AGI benchmark' designed to highlight performance differences in AGI-related tasks. Our findings offer insights into LLMs' capabilities in speculative, interdisciplinary forecasting tasks and emphasize the growing need for innovative evaluation frameworks for assessing AI performance in complex, uncertain real-world scenarios.</p>\"\n",
      "}\n",
      "{\n",
      "  \"title\": \"Benchmarking LLMs for Mimicking Child-Caregiver Language in Interaction\",\n",
      "  \"link\": \"https://papers.cool/arxiv/2412.09318\",\n",
      "  \"description\": \"<a href=\\\"https://arxiv.org/pdf/2412.09318.pdf\\\">[PDF]</a>   <a href=\\\"https://papers.cool/arxiv/2412.09318\\\">[Site]</a>   <a href=\\\"https://papers.cool/arxiv/2412.09318\\\">[Kimi]</a>   <p><b>Authors:</b> Jing Liu</p>   <p>LLMs can generate human-like dialogues, yet their ability to simulate early child-adult interactions remains largely unexplored. In this paper, we examined how effectively LLMs can capture the distinctive features of child-caregiver language in interaction, using both static and interactive benchmarking methods. We found that state-of-the-art LLMs like Llama 3 and GPT-4o can approximate child-caregiver dialogues at the word and utterance level, but they struggle to reproduce the child and caregiver's discursive patterns, exaggerate alignment, and fail to reach the level of diversity shown by humans. The broader goal of this work is to initiate the development of a comprehensive benchmark for LLMs in child-oriented applications.</p>\"\n",
      "}\n",
      "{\n",
      "  \"title\": \"Towards a Multimodal Large Language Model with Pixel-Level Insight for Biomedicine\",\n",
      "  \"link\": \"https://papers.cool/arxiv/2412.09278\",\n",
      "  \"description\": \"<a href=\\\"https://arxiv.org/pdf/2412.09278.pdf\\\">[PDF]</a>   <a href=\\\"https://papers.cool/arxiv/2412.09278\\\">[Site]</a>   <a href=\\\"https://papers.cool/arxiv/2412.09278\\\">[Kimi]</a>   <p><b>Authors:</b> Xiaoshuang Huang</p>   <p>In recent years, Multimodal Large Language Models (MLLM) have achieved notable advancements, demonstrating the feasibility of developing an intelligent biomedical assistant. However, current biomedical MLLMs predominantly focus on image-level understanding and restrict interactions to textual commands, thus limiting their capability boundaries and the flexibility of usage. In this paper, we introduce a novel end-to-end multimodal large language model for the biomedical domain, named MedPLIB, which possesses pixel-level understanding. Excitingly, it supports visual question answering (VQA), arbitrary pixel-level prompts (points, bounding boxes, and free-form shapes), and pixel-level grounding. We propose a novel Mixture-of-Experts (MoE) multi-stage training strategy, which divides MoE into separate training phases for a visual-language expert model and a pixel-grounding expert model, followed by fine-tuning using MoE. This strategy effectively coordinates multitask learning while maintaining the computational cost at inference equivalent to that of a single expert model. To advance the research of biomedical MLLMs, we introduce the Medical Complex Vision Question Answering Dataset (MeCoVQA), which comprises an array of 8 modalities for complex medical imaging question answering and image region understanding. Experimental results indicate that MedPLIB has achieved state-of-the-art outcomes across multiple medical visual language tasks. More importantly, in zero-shot evaluations for the pixel grounding task, MedPLIB leads the best small and large models by margins of 19.7 and 15.6 respectively on the mDice metric. The codes, data, and model checkpoints will be made publicly available at https://github.com/ShawnHuang497/MedPLIB.</p>\"\n",
      "}\n",
      "{\n",
      "  \"title\": \"Towards Understanding the Robustness of LLM-based Evaluations under Perturbations\",\n",
      "  \"link\": \"https://papers.cool/arxiv/2412.09269\",\n",
      "  \"description\": \"<a href=\\\"https://arxiv.org/pdf/2412.09269.pdf\\\">[PDF]</a>   <a href=\\\"https://papers.cool/arxiv/2412.09269\\\">[Site]</a>   <a href=\\\"https://papers.cool/arxiv/2412.09269\\\">[Kimi]</a>   <p><b>Authors:</b> Manav Chaudhary</p>   <p>Traditional evaluation metrics like BLEU and ROUGE fall short when capturing the nuanced qualities of generated text, particularly when there is no single ground truth. In this paper, we explore the potential of Large Language Models (LLMs), specifically Google Gemini 1, to serve as automatic evaluators for non-standardized metrics in summarization and dialog-based tasks. We conduct experiments across multiple prompting strategies to examine how LLMs fare as quality evaluators when compared with human judgments on the SummEval and USR datasets, asking the model to generate both a score as well as a justification for the score. Furthermore, we explore the robustness of the LLM evaluator by using perturbed inputs. Our findings suggest that while LLMs show promise, their alignment with human evaluators is limited, they are not robust against perturbations and significant improvements are required for their standalone use as reliable evaluators for subjective metrics.</p>\"\n",
      "}\n",
      "{\n",
      "  \"title\": \"LMAgent: A Large-scale Multimodal Agents Society for Multi-user Simulation\",\n",
      "  \"link\": \"https://papers.cool/arxiv/2412.09237\",\n",
      "  \"description\": \"<a href=\\\"https://arxiv.org/pdf/2412.09237.pdf\\\">[PDF]</a>   <a href=\\\"https://papers.cool/arxiv/2412.09237\\\">[Site]</a>   <a href=\\\"https://papers.cool/arxiv/2412.09237\\\">[Kimi]</a>   <p><b>Authors:</b> Yijun Liu</p>   <p>The believable simulation of multi-user behavior is crucial for understanding complex social systems. Recently, large language models (LLMs)-based AI agents have made significant progress, enabling them to achieve human-like intelligence across various tasks. However, real human societies are often dynamic and complex, involving numerous individuals engaging in multimodal interactions. In this paper, taking e-commerce scenarios as an example, we present LMAgent, a very large-scale and multimodal agents society based on multimodal LLMs. In LMAgent, besides freely chatting with friends, the agents can autonomously browse, purchase, and review products, even perform live streaming e-commerce. To simulate this complex system, we introduce a self-consistency prompting mechanism to augment agents' multimodal capabilities, resulting in significantly improved decision-making performance over the existing multi-agent system. Moreover, we propose a fast memory mechanism combined with the small-world model to enhance system efficiency, which supports more than 10,000 agent simulations in a society. Experiments on agents' behavior show that these agents achieve comparable performance to humans in behavioral indicators. Furthermore, compared with the existing LLMs-based multi-agent system, more different and valuable phenomena are exhibited, such as herd behavior, which demonstrates the potential of LMAgent in credible large-scale social behavior simulations.</p>\"\n",
      "}\n",
      "{\n",
      "  \"title\": \"When Text Embedding Meets Large Language Model: A Comprehensive Survey\",\n",
      "  \"link\": \"https://papers.cool/arxiv/2412.09165\",\n",
      "  \"description\": \"<a href=\\\"https://arxiv.org/pdf/2412.09165.pdf\\\">[PDF]</a>   <a href=\\\"https://papers.cool/arxiv/2412.09165\\\">[Site]</a>   <a href=\\\"https://papers.cool/arxiv/2412.09165\\\">[Kimi]</a>   <p><b>Authors:</b> Zhijie Nie</p>   <p>Text embedding has become a foundational technology in natural language processing (NLP) during the deep learning era, driving advancements across a wide array of downstream tasks. While many natural language understanding challenges can now be modeled using generative paradigms and leverage the robust generative and comprehension capabilities of large language models (LLMs), numerous practical applications, such as semantic matching, clustering, and information retrieval, continue to rely on text embeddings for their efficiency and effectiveness. In this survey, we categorize the interplay between LLMs and text embeddings into three overarching themes: (1) LLM-augmented text embedding, enhancing traditional embedding methods with LLMs; (2) LLMs as text embedders, utilizing their innate capabilities for embedding generation; and (3) Text embedding understanding with LLMs, leveraging LLMs to analyze and interpret embeddings. By organizing these efforts based on interaction patterns rather than specific downstream applications, we offer a novel and systematic overview of contributions from various research and application domains in the era of LLMs. Furthermore, we highlight the unresolved challenges that persisted in the pre-LLM era with pre-trained language models (PLMs) and explore the emerging obstacles brought forth by LLMs. Building on this analysis, we outline prospective directions for the evolution of text embedding, addressing both theoretical and practical opportunities in the rapidly advancing landscape of NLP.</p>\"\n",
      "}\n",
      "{\n",
      "  \"title\": \"Filter-then-Generate: Large Language Models with Structure-Text Adapter for Knowledge Graph Completion\",\n",
      "  \"link\": \"https://papers.cool/arxiv/2412.09094\",\n",
      "  \"description\": \"<a href=\\\"https://arxiv.org/pdf/2412.09094.pdf\\\">[PDF]</a>   <a href=\\\"https://papers.cool/arxiv/2412.09094\\\">[Site]</a>   <a href=\\\"https://papers.cool/arxiv/2412.09094\\\">[Kimi]</a>   <p><b>Authors:</b> Ben Liu</p>   <p>Large Language Models (LLMs) present massive inherent knowledge and superior semantic comprehension capability, which have revolutionized various tasks in natural language processing. Despite their success, a critical gap remains in enabling LLMs to perform knowledge graph completion (KGC). Empirical evidence suggests that LLMs consistently perform worse than conventional KGC approaches, even through sophisticated prompt design or tailored instruction-tuning. Fundamentally, applying LLMs on KGC introduces several critical challenges, including a vast set of entity candidates, hallucination issue of LLMs, and under-exploitation of the graph structure. To address these challenges, we propose a novel instruction-tuning-based method, namely FtG. Specifically, we present a \\\\textit{filter-then-generate} paradigm and formulate the KGC task into a multiple-choice question format. In this way, we can harness the capability of LLMs while mitigating the issue casused by hallucinations. Moreover, we devise a flexible ego-graph serialization prompt and employ a structure-text adapter to couple structure and text information in a contextualized manner. Experimental results demonstrate that FtG achieves substantial performance gain compared to existing state-of-the-art methods. The instruction dataset and code are available at \\\\url{https://github.com/LB0828/FtG}.</p>\"\n",
      "}\n",
      "{\n",
      "  \"title\": \"Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning\",\n",
      "  \"link\": \"https://papers.cool/arxiv/2412.09078\",\n",
      "  \"description\": \"<a href=\\\"https://arxiv.org/pdf/2412.09078.pdf\\\">[PDF]</a>   <a href=\\\"https://papers.cool/arxiv/2412.09078\\\">[Site]</a>   <a href=\\\"https://papers.cool/arxiv/2412.09078\\\">[Kimi]</a>   <p><b>Authors:</b> Zhenni Bi</p>   <p>Large Language Models (LLMs) have shown remarkable abilities across various language tasks, but solving complex reasoning problems remains a challenge. While existing methods like Chain-of-Thought (CoT) and Tree-of-Thought (ToT) enhance reasoning by decomposing problems or structuring prompts, they typically perform a single pass of reasoning and may fail to revisit flawed paths, compromising accuracy. To address this, we propose a novel reasoning framework called Forest-of-Thought (FoT), which integrates multiple reasoning trees to leverage collective decision-making for solving complex logical problems. FoT utilizes sparse activation strategies to select the most relevant reasoning paths, improving both efficiency and accuracy. Additionally, we introduce a dynamic self-correction strategy that enables real-time error correction and learning from past mistakes, as well as consensus-guided decision making strategies to optimize correctness and computational resources. Experimental results demonstrate that the FoT framework, combined with these strategies, significantly enhances the reasoning capabilities of LLMs, enabling them to solve complex tasks with greater precision and efficiency.</p>\"\n",
      "}\n",
      "{\n",
      "  \"title\": \"EmbedGenius: Towards Automated Software Development for Generic Embedded IoT Systems\",\n",
      "  \"link\": \"https://papers.cool/arxiv/2412.09058\",\n",
      "  \"description\": \"<a href=\\\"https://arxiv.org/pdf/2412.09058.pdf\\\">[PDF]</a>   <a href=\\\"https://papers.cool/arxiv/2412.09058\\\">[Site]</a>   <a href=\\\"https://papers.cool/arxiv/2412.09058\\\">[Kimi]</a>   <p><b>Authors:</b> Huanqi Yang</p>   <p>Embedded IoT system development is crucial for enabling seamless connectivity and functionality across a wide range of applications. However, such a complex process requires cross-domain knowledge of hardware and software and hence often necessitates direct developer involvement, making it labor-intensive, time-consuming, and error-prone. To address this challenge, this paper introduces EmbedGenius, the first fully automated software development platform for general-purpose embedded IoT systems. The key idea is to leverage the reasoning ability of Large Language Models (LLMs) and embedded system expertise to automate the hardware-in-the-loop development process. The main methods include a component-aware library resolution method for addressing hardware dependencies, a library knowledge generation method that injects utility domain knowledge into LLMs, and an auto-programming method that ensures successful deployment. We evaluate EmbedGenius's performance across 71 modules and four mainstream embedded development platforms with over 350 IoT tasks. Experimental results show that EmbedGenius can generate codes with an accuracy of 95.7% and complete tasks with a success rate of 86.5%, surpassing human-in-the-loop baselines by 15.6%--37.7% and 25.5%--53.4%, respectively. We also show EmbedGenius's potential through case studies in environmental monitoring and remote control systems development.</p>\"\n",
      "}\n",
      "{\n",
      "  \"title\": \"Multi-Task Learning with LLMs for Implicit Sentiment Analysis: Data-level and Task-level Automatic Weight Learning\",\n",
      "  \"link\": \"https://papers.cool/arxiv/2412.09046\",\n",
      "  \"description\": \"<a href=\\\"https://arxiv.org/pdf/2412.09046.pdf\\\">[PDF]</a>   <a href=\\\"https://papers.cool/arxiv/2412.09046\\\">[Site]</a>   <a href=\\\"https://papers.cool/arxiv/2412.09046\\\">[Kimi]</a>   <p><b>Authors:</b> Wenna Lai</p>   <p>Implicit sentiment analysis (ISA) presents significant challenges due to the absence of salient cue words. Previous methods have struggled with insufficient data and limited reasoning capabilities to infer underlying opinions. Integrating multi-task learning (MTL) with large language models (LLMs) offers the potential to enable models of varying sizes to reliably perceive and recognize genuine opinions in ISA. However, existing MTL approaches are constrained by two sources of uncertainty: data-level uncertainty, arising from hallucination problems in LLM-generated contextual information, and task-level uncertainty, stemming from the varying capacities of models to process contextual information. To handle these uncertainties, we introduce MT-ISA, a novel MTL framework that enhances ISA by leveraging the generation and reasoning capabilities of LLMs through automatic MTL. Specifically, MT-ISA constructs auxiliary tasks using generative LLMs to supplement sentiment elements and incorporates automatic MTL to fully exploit auxiliary data. We introduce data-level and task-level automatic weight learning (AWL), which dynamically identifies relationships and prioritizes more reliable data and critical tasks, enabling models of varying sizes to adaptively learn fine-grained weights based on their reasoning capabilities. We investigate three strategies for data-level AWL, while also introducing homoscedastic uncertainty for task-level AWL. Extensive experiments reveal that models of varying sizes achieve an optimal balance between primary prediction and auxiliary tasks in MT-ISA. This underscores the effectiveness and adaptability of our approach.</p>\"\n",
      "}\n",
      "{\n",
      "  \"title\": \"What Makes Cryptic Crosswords Challenging for LLMs?\",\n",
      "  \"link\": \"https://papers.cool/arxiv/2412.09012\",\n",
      "  \"description\": \"<a href=\\\"https://arxiv.org/pdf/2412.09012.pdf\\\">[PDF]</a>   <a href=\\\"https://papers.cool/arxiv/2412.09012\\\">[Site]</a>   <a href=\\\"https://papers.cool/arxiv/2412.09012\\\">[Kimi]</a>   <p><b>Authors:</b> Abdelrahman Sadallah</p>   <p>Cryptic crosswords are puzzles that rely on general knowledge and the solver's ability to manipulate language on different levels, dealing with various types of wordplay. Previous research suggests that solving such puzzles is challenging even for modern NLP models, including Large Language Models (LLMs). However, there is little to no research on the reasons for their poor performance on this task. In this paper, we establish the benchmark results for three popular LLMs: Gemma2, LLaMA3 and ChatGPT, showing that their performance on this task is still significantly below that of humans. We also investigate why these models struggle to achieve superior performance. We release our code and introduced datasets at https://github.com/bodasadallah/decrypting-crosswords.</p>\"\n",
      "}\n",
      "{\n",
      "  \"title\": \"RuleArena: A Benchmark for Rule-Guided Reasoning with LLMs in Real-World Scenarios\",\n",
      "  \"link\": \"https://papers.cool/arxiv/2412.08972\",\n",
      "  \"description\": \"<a href=\\\"https://arxiv.org/pdf/2412.08972.pdf\\\">[PDF]</a>   <a href=\\\"https://papers.cool/arxiv/2412.08972\\\">[Site]</a>   <a href=\\\"https://papers.cool/arxiv/2412.08972\\\">[Kimi]</a>   <p><b>Authors:</b> Ruiwen Zhou</p>   <p>This paper introduces RuleArena, a novel and challenging benchmark designed to evaluate the ability of large language models (LLMs) to follow complex, real-world rules in reasoning. Covering three practical domains -- airline baggage fees, NBA transactions, and tax regulations -- RuleArena assesses LLMs' proficiency in handling intricate natural language instructions that demand long-context understanding, logical reasoning, and accurate mathematical computation. Two key attributes distinguish RuleArena from traditional rule-based reasoning benchmarks: (1) it extends beyond standard first-order logic representations, and (2) it is grounded in authentic, practical scenarios, providing insights into the suitability and reliability of LLMs for real-world applications. Our findings reveal several notable limitations in LLMs: (1) they struggle to identify and apply the appropriate rules, frequently becoming confused by similar but distinct regulations, (2) they cannot consistently perform accurate mathematical computations, even when they correctly identify the relevant rules, and (3) in general, they perform poorly in the benchmark. These results highlight significant challenges in advancing LLMs' rule-guided reasoning capabilities in real-life applications.</p>\"\n",
      "}\n",
      "{\n",
      "  \"title\": \"Exploring Large Language Models on Cross-Cultural Values in Connection with Training Methodology\",\n",
      "  \"link\": \"https://papers.cool/arxiv/2412.08846\",\n",
      "  \"description\": \"<a href=\\\"https://arxiv.org/pdf/2412.08846.pdf\\\">[PDF]</a>   <a href=\\\"https://papers.cool/arxiv/2412.08846\\\">[Site]</a>   <a href=\\\"https://papers.cool/arxiv/2412.08846\\\">[Kimi]</a>   <p><b>Authors:</b> Minsang Kim</p>   <p>Large language models (LLMs) closely interact with humans, and thus need an intimate understanding of the cultural values of human society. In this paper, we explore how open-source LLMs make judgments on diverse categories of cultural values across countries, and its relation to training methodology such as model sizes, training corpus, alignment, etc. Our analysis shows that LLMs can judge socio-cultural norms similar to humans but less so on social systems and progress. In addition, LLMs tend to judge cultural values biased toward Western culture, which can be improved with training on the multilingual corpus. We also find that increasing model size helps a better understanding of social values, but smaller models can be enhanced by using synthetic data. Our analysis reveals valuable insights into the design methodology of LLMs in connection with their understanding of cultural values.</p>\"\n",
      "}\n",
      "{\n",
      "  \"title\": \"Kajal: Extracting Grammar of a Source Code Using Large Language Models\",\n",
      "  \"link\": \"https://papers.cool/arxiv/2412.08842\",\n",
      "  \"description\": \"<a href=\\\"https://arxiv.org/pdf/2412.08842.pdf\\\">[PDF]</a>   <a href=\\\"https://papers.cool/arxiv/2412.08842\\\">[Site]</a>   <a href=\\\"https://papers.cool/arxiv/2412.08842\\\">[Kimi]</a>   <p><b>Authors:</b> Mohammad Jalili Torkamani</p>   <p>Understanding and extracting the grammar of a domain-specific language (DSL) is crucial for various software engineering tasks; however, manually creating these grammars is time-intensive and error-prone. This paper presents Kajal, a novel approach that automatically infers grammar from DSL code snippets by leveraging Large Language Models (LLMs) through prompt engineering and few-shot learning. Kajal dynamically constructs input prompts, using contextual information to guide the LLM in generating the corresponding grammars, which are iteratively refined through a feedback-driven approach. Our experiments show that Kajal achieves 60% accuracy with few-shot learning and 45% without it, demonstrating the significant impact of few-shot learning on the tool's effectiveness. This approach offers a promising solution for automating DSL grammar extraction, and future work will explore using smaller, open-source LLMs and testing on larger datasets to further validate Kajal's performance.</p>\"\n",
      "}\n",
      "{\n",
      "  \"title\": \"Autoformalizing and Simulating Game-Theoretic Scenarios using LLM-augmented Agents\",\n",
      "  \"link\": \"https://papers.cool/arxiv/2412.08805\",\n",
      "  \"description\": \"<a href=\\\"https://arxiv.org/pdf/2412.08805.pdf\\\">[PDF]</a>   <a href=\\\"https://papers.cool/arxiv/2412.08805\\\">[Site]</a>   <a href=\\\"https://papers.cool/arxiv/2412.08805\\\">[Kimi]</a>   <p><b>Authors:</b> Agnieszka Mensfelt</p>   <p>Game-theoretic simulations are a versatile tool for exploring interactions of both natural and artificial agents. However, modelling real-world scenarios and developing simulations often require substantial human expertise and effort. To streamline this process, we present a framework that enables the autoformalization of game-theoretic scenarios using agents augmented by large language models (LLMs). In this approach, LLM-augmented agents translate natural language scenario descriptions into executable logic programs that define the rules of each game, validating these programs for syntactic accuracy. A tournament simulation is then conducted, during which the agents test the functionality of the generated games by playing them. When a ground truth payoff matrix is available, an exact semantic validation can also be performed. The validated games can then be used in further simulations to assess the effectiveness of different strategies. We evaluate our approach on a diverse set of 55 natural language descriptions across five well-known 2x2 simultaneous-move games, demonstrating 96% syntactic and 87% semantic correctness in the generated game rules. Additionally, we assess the LLM-augmented agents' capability to autoformalize strategies for gameplay.</p>\"\n",
      "}\n",
      "{\n",
      "  \"title\": \"Coverage-based Fairness in Multi-document Summarization\",\n",
      "  \"link\": \"https://papers.cool/arxiv/2412.08795\",\n",
      "  \"description\": \"<a href=\\\"https://arxiv.org/pdf/2412.08795.pdf\\\">[PDF]</a>   <a href=\\\"https://papers.cool/arxiv/2412.08795\\\">[Site]</a>   <a href=\\\"https://papers.cool/arxiv/2412.08795\\\">[Kimi]</a>   <p><b>Authors:</b> Haoyuan Li</p>   <p>Fairness in multi-document summarization (MDS) measures whether a system can generate a summary fairly representing information from documents with different social attribute values. Fairness in MDS is crucial since a fair summary can offer readers a comprehensive view. Previous works focus on quantifying summary-level fairness using Proportional Representation, a fairness measure based on Statistical Parity. However, Proportional Representation does not consider redundancy in input documents and overlooks corpus-level unfairness. In this work, we propose a new summary-level fairness measure, Equal Coverage, which is based on coverage of documents with different social attribute values and considers the redundancy within documents. To detect the corpus-level unfairness, we propose a new corpus-level measure, Coverage Parity. Our human evaluations show that our measures align more with our definition of fairness. Using our measures, we evaluate the fairness of thirteen different LLMs. We find that Claude3-sonnet is the fairest among all evaluated LLMs. We also find that almost all LLMs overrepresent different social attribute values.</p>\"\n",
      "}\n",
      "{\n",
      "  \"title\": \"LLaVA-Zip: Adaptive Visual Token Compression with Intrinsic Image Information\",\n",
      "  \"link\": \"https://papers.cool/arxiv/2412.08771\",\n",
      "  \"description\": \"<a href=\\\"https://arxiv.org/pdf/2412.08771.pdf\\\">[PDF]</a>   <a href=\\\"https://papers.cool/arxiv/2412.08771\\\">[Site]</a>   <a href=\\\"https://papers.cool/arxiv/2412.08771\\\">[Kimi]</a>   <p><b>Authors:</b> Ke Wang</p>   <p>Multi-modal large language models (MLLMs) utilizing instruction-following data, such as LLaVA, have achieved great progress in the industry. A major limitation in these models is that visual tokens consume a substantial portion of the maximum token limit in large language models (LLMs), leading to increased computational demands and decreased performance when prompts include multiple images or videos. Industry solutions often mitigate this issue by increasing computational power, but this approach is less feasible in academic environments with limited resources. In this study, we propose Dynamic Feature Map Reduction (DFMR) based on LLaVA-1.5 to address the challenge of visual token overload. DFMR dynamically compresses the visual tokens, freeing up token capacity. Our experimental results demonstrate that integrating DFMR into LLaVA-1.5 significantly improves the performance of LLaVA in varied visual token lengths, offering a promising solution for extending LLaVA to handle multi-image and video scenarios in resource-constrained academic environments and it can also be applied in industry settings for data augmentation to help mitigate the scarcity of open-domain image-text pair datasets in the continued pretraining stage.</p>\"\n",
      "}\n",
      "{\n",
      "  \"title\": \"In-Context Learning with Topological Information for Knowledge Graph Completion\",\n",
      "  \"link\": \"https://papers.cool/arxiv/2412.08742\",\n",
      "  \"description\": \"<a href=\\\"https://arxiv.org/pdf/2412.08742.pdf\\\">[PDF]</a>   <a href=\\\"https://papers.cool/arxiv/2412.08742\\\">[Site]</a>   <a href=\\\"https://papers.cool/arxiv/2412.08742\\\">[Kimi]</a>   <p><b>Authors:</b> Udari Madhushani Sehwag</p>   <p>Knowledge graphs (KGs) are crucial for representing and reasoning over structured information, supporting a wide range of applications such as information retrieval, question answering, and decision-making. However, their effectiveness is often hindered by incompleteness, limiting their potential for real-world impact. While knowledge graph completion (KGC) has been extensively studied in the literature, recent advances in generative AI models, particularly large language models (LLMs), have introduced new opportunities for innovation. In-context learning has recently emerged as a promising approach for leveraging pretrained knowledge of LLMs across a range of natural language processing tasks and has been widely adopted in both academia and industry. However, how to utilize in-context learning for effective KGC remains relatively underexplored. We develop a novel method that incorporates topological information through in-context learning to enhance KGC performance. By integrating ontological knowledge and graph structure into the context of LLMs, our approach achieves strong performance in the transductive setting i.e., nodes in the test graph dataset are present in the training graph dataset. Furthermore, we apply our approach to KGC in the more challenging inductive setting, i.e., nodes in the training graph dataset and test graph dataset are disjoint, leveraging the ontology to infer useful information about missing nodes which serve as contextual cues for the LLM during inference. Our method demonstrates superior performance compared to baselines on the ILPC-small and ILPC-large datasets.</p>\"\n",
      "}\n",
      "{\n",
      "  \"title\": \"Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions\",\n",
      "  \"link\": \"https://papers.cool/arxiv/2412.08737\",\n",
      "  \"description\": \"<a href=\\\"https://arxiv.org/pdf/2412.08737.pdf\\\">[PDF]</a>   <a href=\\\"https://papers.cool/arxiv/2412.08737\\\">[Site]</a>   <a href=\\\"https://papers.cool/arxiv/2412.08737\\\">[Kimi]</a>   <p><b>Authors:</b> Jiarui Zhang</p>   <p>Multimodal large language models (MLLMs) have made rapid progress in recent years, yet continue to struggle with low-level visual perception (LLVP) -- particularly the ability to accurately describe the geometric details of an image. This capability is crucial for applications in areas such as robotics, medical image analysis, and manufacturing. In this paper, we first introduce Geoperception, a benchmark designed to evaluate an MLLM's ability to accurately transcribe 2D geometric information from an image. Using this benchmark, we demonstrate the limitations of leading MLLMs, and then conduct a comprehensive empirical study to explore strategies for improving their performance on geometric tasks. Our findings highlight the benefits of certain model architectures, training techniques, and data strategies, including the use of high-fidelity synthetic data and multi-stage training with a data curriculum. Notably, we find that a data curriculum enables models to learn challenging geometry understanding tasks which they fail to learn from scratch. Leveraging these insights, we develop Euclid, a family of models specifically optimized for strong low-level geometric perception. Although purely trained on synthetic multimodal data, Euclid shows strong generalization ability to novel geometry shapes. For instance, Euclid outperforms the best closed-source model, Gemini-1.5-Pro, by up to 58.56% on certain Geoperception benchmark tasks and 10.65% on average across all tasks.</p>\"\n",
      "}\n",
      "{\n",
      "  \"title\": \"Distinguishing Scams and Fraud with Ensemble Learning\",\n",
      "  \"link\": \"https://papers.cool/arxiv/2412.08680\",\n",
      "  \"description\": \"<a href=\\\"https://arxiv.org/pdf/2412.08680.pdf\\\">[PDF]</a>   <a href=\\\"https://papers.cool/arxiv/2412.08680\\\">[Site]</a>   <a href=\\\"https://papers.cool/arxiv/2412.08680\\\">[Kimi]</a>   <p><b>Authors:</b> Isha Chadalavada</p>   <p>Users increasingly query LLM-enabled web chatbots for help with scam defense. The Consumer Financial Protection Bureau's complaints database is a rich data source for evaluating LLM performance on user scam queries, but currently the corpus does not distinguish between scam and non-scam fraud. We developed an LLM ensemble approach to distinguishing scam and fraud CFPB complaints and describe initial findings regarding the strengths and weaknesses of LLMs in the scam defense context.</p>\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import feedparser\n",
    "import json\n",
    "\n",
    "# RSS源URL\n",
    "rss_url = 'http://zmdsn.fun/rss/papers/arxiv/cs.AI'\n",
    "\n",
    "# 要筛选的关键字列表\n",
    "keywords = ['llm']\n",
    "\n",
    "# 解析RSS源\n",
    "feed = feedparser.parse(rss_url)\n",
    "\n",
    "# 筛选包含关键字的RSS项\n",
    "filtered_entries = []\n",
    "for entry in feed.entries:\n",
    "    if any(keyword in entry.title.lower() or keyword in entry.description.lower() for keyword in keywords):\n",
    "        filtered_entries.append({\n",
    "            'title': entry.title,\n",
    "            'link': entry.link,\n",
    "            'description': entry.description\n",
    "        })\n",
    "\n",
    "# 转发筛选后的RSS项\n",
    "for entry in filtered_entries:\n",
    "    print(json.dumps(entry, ensure_ascii=False, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'redis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# rss_url = 'http://zmdsn.fun/rss/papers/arxiv/cs.AI'\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# feed\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# feedparser.parse(rss_url)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mredis\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'redis'"
     ]
    }
   ],
   "source": [
    "# rss_url = 'http://zmdsn.fun/rss/papers/arxiv/cs.AI'\n",
    "# feed\n",
    "# feedparser.parse(rss_url)\n",
    "\n",
    "import redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(feedparser.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "DataError",
     "evalue": "Invalid input of type: 'FeedParserDict'. Convert to a bytes, string, int or float first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDataError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 32\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m filtered_entries:\n\u001b[1;32m     29\u001b[0m         redis_client\u001b[38;5;241m.\u001b[39mlpush(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm agv\u001b[39m\u001b[38;5;124m\"\u001b[39m, entry)\n\u001b[0;32m---> 32\u001b[0m get_rss(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm_agv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[44], line 29\u001b[0m, in \u001b[0;36mget_rss\u001b[0;34m(keyword_str)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# 保存筛选后的RSS项到Redis\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m filtered_entries:\n\u001b[0;32m---> 29\u001b[0m     redis_client\u001b[38;5;241m.\u001b[39mlpush(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm agv\u001b[39m\u001b[38;5;124m\"\u001b[39m, entry)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/redis/commands/core.py:2728\u001b[0m, in \u001b[0;36mListCommands.lpush\u001b[0;34m(self, name, *values)\u001b[0m\n\u001b[1;32m   2722\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlpush\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39mvalues: FieldT) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Awaitable[\u001b[38;5;28mint\u001b[39m], \u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m   2723\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2724\u001b[0m \u001b[38;5;124;03m    Push ``values`` onto the head of the list ``name``\u001b[39;00m\n\u001b[1;32m   2725\u001b[0m \n\u001b[1;32m   2726\u001b[0m \u001b[38;5;124;03m    For more information see https://redis.io/commands/lpush\u001b[39;00m\n\u001b[1;32m   2727\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2728\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute_command(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLPUSH\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, \u001b[38;5;241m*\u001b[39mvalues)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/redis/client.py:559\u001b[0m, in \u001b[0;36mRedis.execute_command\u001b[0;34m(self, *args, **options)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions):\n\u001b[0;32m--> 559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_command(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/redis/client.py:567\u001b[0m, in \u001b[0;36mRedis._execute_command\u001b[0;34m(self, *args, **options)\u001b[0m\n\u001b[1;32m    565\u001b[0m conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection \u001b[38;5;129;01mor\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mget_connection(command_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mretry\u001b[38;5;241m.\u001b[39mcall_with_retry(\n\u001b[1;32m    568\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_command_parse_response(\n\u001b[1;32m    569\u001b[0m             conn, command_name, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m    570\u001b[0m         ),\n\u001b[1;32m    571\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m error: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disconnect_raise(conn, error),\n\u001b[1;32m    572\u001b[0m     )\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/redis/retry.py:62\u001b[0m, in \u001b[0;36mRetry.call_with_retry\u001b[0;34m(self, do, fail)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 62\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m do()\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_supported_errors \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m     64\u001b[0m         failures \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/redis/client.py:568\u001b[0m, in \u001b[0;36mRedis._execute_command.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    565\u001b[0m conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection \u001b[38;5;129;01mor\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mget_connection(command_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mretry\u001b[38;5;241m.\u001b[39mcall_with_retry(\n\u001b[0;32m--> 568\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_command_parse_response(\n\u001b[1;32m    569\u001b[0m             conn, command_name, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m    570\u001b[0m         ),\n\u001b[1;32m    571\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m error: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disconnect_raise(conn, error),\n\u001b[1;32m    572\u001b[0m     )\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/redis/client.py:541\u001b[0m, in \u001b[0;36mRedis._send_command_parse_response\u001b[0;34m(self, conn, command_name, *args, **options)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_send_command_parse_response\u001b[39m(\u001b[38;5;28mself\u001b[39m, conn, command_name, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions):\n\u001b[1;32m    538\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;124;03m    Send a command and parse the response\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 541\u001b[0m     conn\u001b[38;5;241m.\u001b[39msend_command(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_response(conn, command_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/redis/connection.py:557\u001b[0m, in \u001b[0;36mAbstractConnection.send_command\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    555\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Pack and send a command to the Redis server\"\"\"\u001b[39;00m\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend_packed_command(\n\u001b[0;32m--> 557\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command_packer\u001b[38;5;241m.\u001b[39mpack(\u001b[38;5;241m*\u001b[39margs),\n\u001b[1;32m    558\u001b[0m         check_health\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheck_health\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m    559\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/redis/connection.py:107\u001b[0m, in \u001b[0;36mPythonRespSerializer.pack\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    104\u001b[0m buff \u001b[38;5;241m=\u001b[39m SYM_EMPTY\u001b[38;5;241m.\u001b[39mjoin((SYM_STAR, \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(args))\u001b[38;5;241m.\u001b[39mencode(), SYM_CRLF))\n\u001b[1;32m    106\u001b[0m buffer_cutoff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer_cutoff\n\u001b[0;32m--> 107\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode, args):\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m# to avoid large string mallocs, chunk the command into the\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;66;03m# output list if we're sending large values or memoryviews\u001b[39;00m\n\u001b[1;32m    110\u001b[0m     arg_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(arg)\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;28mlen\u001b[39m(buff) \u001b[38;5;241m>\u001b[39m buffer_cutoff\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m arg_length \u001b[38;5;241m>\u001b[39m buffer_cutoff\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, \u001b[38;5;28mmemoryview\u001b[39m)\n\u001b[1;32m    115\u001b[0m     ):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/redis/_parsers/encoders.py:29\u001b[0m, in \u001b[0;36mEncoder.encode\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# a value we don't know how to deal with. throw an error\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     typename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DataError(\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input of type: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtypename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert to a bytes, string, int or float first.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     32\u001b[0m     )\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     34\u001b[0m     value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding_errors)\n",
      "\u001b[0;31mDataError\u001b[0m: Invalid input of type: 'FeedParserDict'. Convert to a bytes, string, int or float first."
     ]
    }
   ],
   "source": [
    "import feedparser\n",
    "import json\n",
    "from flask import Flask, jsonify\n",
    "import redis\n",
    "\n",
    "def filtered_entries(keyword_str):\n",
    "    rss_url = 'http://zmdsn.fun/rss/papers/arxiv/cs.AI'\n",
    "    keywords = keyword_str.lower().split(\"_\")\n",
    "\n",
    "    feed = feedparser.parse(rss_url)\n",
    "\n",
    "    filtered_entries = []\n",
    "    for entry in feed.entries:\n",
    "        if any(keyword in entry.title.lower() or keyword in entry.description.lower() for keyword in keywords):\n",
    "            filtered_entries.append(entry)\n",
    "\n",
    "    return filtered_entries\n",
    "\n",
    "def save_rss(keyword_str):\n",
    "    filtered_entries = filtered_entries(keyword_str)\n",
    "    redis_host = 'localhost'\n",
    "    redis_port = 6379\n",
    "    redis_password = None  # 如果设置了Redis密码，请在这里提供\n",
    "    redis_client = redis.Redis(host=redis_host, port=redis_port, password=redis_password)\n",
    "    for entry in filtered_entries:\n",
    "        rss_str = json.dumps(entry)\n",
    "        rss_lists = redis_client.lrange(keyword_str, 0, -1)\n",
    "        if rss_str not in rss_lists:\n",
    "            redis_client.lpush(keyword_str, json.dumps(entry))\n",
    "\n",
    "\n",
    "save_rss(\"llm_agv\")\n",
    "\n",
    "# 获取筛选后的RSS项\n",
    "# @app.route('/arxiv/<keywords>', methods=['GET'])\n",
    "# def get_rss(keywords):\n",
    "#     get_rss()\n",
    "#     entries = []\n",
    "#     for entry_json in redis_client.lrange('filtered_rss', 0, -1):\n",
    "#         entries.append(json.loads(entry_json))\n",
    "#     return jsonify(entries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'{\"title\": \"Distinguishing Scams and Fraud with Ensemble Learning\", \"link\": \"https://papers.cool/arxiv/2412.08680\", \"description\": \"<a href=\\\\\"https://arxiv.org/pdf/2412.08680.pdf\\\\\">[PDF]</a>   <a href=\\\\\"https://papers.cool/arxiv/2412.08680\\\\\">[Site]</a>   <a href=\\\\\"https://papers.cool/arxiv/2412.08680\\\\\">[Kimi]</a>   <p><b>Authors:</b> Isha Chadalavada</p>   <p>Users increasingly query LLM-enabled web chatbots for help with scam defense. The Consumer Financial Protection Bureau\\'s complaints database is a rich data source for evaluating LLM performance on user scam queries, but currently the corpus does not distinguish between scam and non-scam fraud. We developed an LLM ensemble approach to distinguishing scam and fraud CFPB complaints and describe initial findings regarding the strengths and weaknesses of LLMs in the scam defense context.</p>\"}',\n",
       " b'{\"title\": \"Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions\", \"link\": \"https://papers.cool/arxiv/2412.08737\", \"description\": \"<a href=\\\\\"https://arxiv.org/pdf/2412.08737.pdf\\\\\">[PDF]</a>   <a href=\\\\\"https://papers.cool/arxiv/2412.08737\\\\\">[Site]</a>   <a href=\\\\\"https://papers.cool/arxiv/2412.08737\\\\\">[Kimi]</a>   <p><b>Authors:</b> Jiarui Zhang</p>   <p>Multimodal large language models (MLLMs) have made rapid progress in recent years, yet continue to struggle with low-level visual perception (LLVP) -- particularly the ability to accurately describe the geometric details of an image. This capability is crucial for applications in areas such as robotics, medical image analysis, and manufacturing. In this paper, we first introduce Geoperception, a benchmark designed to evaluate an MLLM\\'s ability to accurately transcribe 2D geometric information from an image. Using this benchmark, we demonstrate the limitations of leading MLLMs, and then conduct a comprehensive empirical study to explore strategies for improving their performance on geometric tasks. Our findings highlight the benefits of certain model architectures, training techniques, and data strategies, including the use of high-fidelity synthetic data and multi-stage training with a data curriculum. Notably, we find that a data curriculum enables models to learn challenging geometry understanding tasks which they fail to learn from scratch. Leveraging these insights, we develop Euclid, a family of models specifically optimized for strong low-level geometric perception. Although purely trained on synthetic multimodal data, Euclid shows strong generalization ability to novel geometry shapes. For instance, Euclid outperforms the best closed-source model, Gemini-1.5-Pro, by up to 58.56% on certain Geoperception benchmark tasks and 10.65% on average across all tasks.</p>\"}',\n",
       " b'{\"title\": \"In-Context Learning with Topological Information for Knowledge Graph Completion\", \"link\": \"https://papers.cool/arxiv/2412.08742\", \"description\": \"<a href=\\\\\"https://arxiv.org/pdf/2412.08742.pdf\\\\\">[PDF]</a>   <a href=\\\\\"https://papers.cool/arxiv/2412.08742\\\\\">[Site]</a>   <a href=\\\\\"https://papers.cool/arxiv/2412.08742\\\\\">[Kimi]</a>   <p><b>Authors:</b> Udari Madhushani Sehwag</p>   <p>Knowledge graphs (KGs) are crucial for representing and reasoning over structured information, supporting a wide range of applications such as information retrieval, question answering, and decision-making. However, their effectiveness is often hindered by incompleteness, limiting their potential for real-world impact. While knowledge graph completion (KGC) has been extensively studied in the literature, recent advances in generative AI models, particularly large language models (LLMs), have introduced new opportunities for innovation. In-context learning has recently emerged as a promising approach for leveraging pretrained knowledge of LLMs across a range of natural language processing tasks and has been widely adopted in both academia and industry. However, how to utilize in-context learning for effective KGC remains relatively underexplored. We develop a novel method that incorporates topological information through in-context learning to enhance KGC performance. By integrating ontological knowledge and graph structure into the context of LLMs, our approach achieves strong performance in the transductive setting i.e., nodes in the test graph dataset are present in the training graph dataset. Furthermore, we apply our approach to KGC in the more challenging inductive setting, i.e., nodes in the training graph dataset and test graph dataset are disjoint, leveraging the ontology to infer useful information about missing nodes which serve as contextual cues for the LLM during inference. Our method demonstrates superior performance compared to baselines on the ILPC-small and ILPC-large datasets.</p>\"}',\n",
       " b'{\"title\": \"LLaVA-Zip: Adaptive Visual Token Compression with Intrinsic Image Information\", \"link\": \"https://papers.cool/arxiv/2412.08771\", \"description\": \"<a href=\\\\\"https://arxiv.org/pdf/2412.08771.pdf\\\\\">[PDF]</a>   <a href=\\\\\"https://papers.cool/arxiv/2412.08771\\\\\">[Site]</a>   <a href=\\\\\"https://papers.cool/arxiv/2412.08771\\\\\">[Kimi]</a>   <p><b>Authors:</b> Ke Wang</p>   <p>Multi-modal large language models (MLLMs) utilizing instruction-following data, such as LLaVA, have achieved great progress in the industry. A major limitation in these models is that visual tokens consume a substantial portion of the maximum token limit in large language models (LLMs), leading to increased computational demands and decreased performance when prompts include multiple images or videos. Industry solutions often mitigate this issue by increasing computational power, but this approach is less feasible in academic environments with limited resources. In this study, we propose Dynamic Feature Map Reduction (DFMR) based on LLaVA-1.5 to address the challenge of visual token overload. DFMR dynamically compresses the visual tokens, freeing up token capacity. Our experimental results demonstrate that integrating DFMR into LLaVA-1.5 significantly improves the performance of LLaVA in varied visual token lengths, offering a promising solution for extending LLaVA to handle multi-image and video scenarios in resource-constrained academic environments and it can also be applied in industry settings for data augmentation to help mitigate the scarcity of open-domain image-text pair datasets in the continued pretraining stage.</p>\"}',\n",
       " b'{\"title\": \"Coverage-based Fairness in Multi-document Summarization\", \"link\": \"https://papers.cool/arxiv/2412.08795\", \"description\": \"<a href=\\\\\"https://arxiv.org/pdf/2412.08795.pdf\\\\\">[PDF]</a>   <a href=\\\\\"https://papers.cool/arxiv/2412.08795\\\\\">[Site]</a>   <a href=\\\\\"https://papers.cool/arxiv/2412.08795\\\\\">[Kimi]</a>   <p><b>Authors:</b> Haoyuan Li</p>   <p>Fairness in multi-document summarization (MDS) measures whether a system can generate a summary fairly representing information from documents with different social attribute values. Fairness in MDS is crucial since a fair summary can offer readers a comprehensive view. Previous works focus on quantifying summary-level fairness using Proportional Representation, a fairness measure based on Statistical Parity. However, Proportional Representation does not consider redundancy in input documents and overlooks corpus-level unfairness. In this work, we propose a new summary-level fairness measure, Equal Coverage, which is based on coverage of documents with different social attribute values and considers the redundancy within documents. To detect the corpus-level unfairness, we propose a new corpus-level measure, Coverage Parity. Our human evaluations show that our measures align more with our definition of fairness. Using our measures, we evaluate the fairness of thirteen different LLMs. We find that Claude3-sonnet is the fairest among all evaluated LLMs. We also find that almost all LLMs overrepresent different social attribute values.</p>\"}',\n",
       " b'{\"title\": \"Autoformalizing and Simulating Game-Theoretic Scenarios using LLM-augmented Agents\", \"link\": \"https://papers.cool/arxiv/2412.08805\", \"description\": \"<a href=\\\\\"https://arxiv.org/pdf/2412.08805.pdf\\\\\">[PDF]</a>   <a href=\\\\\"https://papers.cool/arxiv/2412.08805\\\\\">[Site]</a>   <a href=\\\\\"https://papers.cool/arxiv/2412.08805\\\\\">[Kimi]</a>   <p><b>Authors:</b> Agnieszka Mensfelt</p>   <p>Game-theoretic simulations are a versatile tool for exploring interactions of both natural and artificial agents. However, modelling real-world scenarios and developing simulations often require substantial human expertise and effort. To streamline this process, we present a framework that enables the autoformalization of game-theoretic scenarios using agents augmented by large language models (LLMs). In this approach, LLM-augmented agents translate natural language scenario descriptions into executable logic programs that define the rules of each game, validating these programs for syntactic accuracy. A tournament simulation is then conducted, during which the agents test the functionality of the generated games by playing them. When a ground truth payoff matrix is available, an exact semantic validation can also be performed. The validated games can then be used in further simulations to assess the effectiveness of different strategies. We evaluate our approach on a diverse set of 55 natural language descriptions across five well-known 2x2 simultaneous-move games, demonstrating 96% syntactic and 87% semantic correctness in the generated game rules. Additionally, we assess the LLM-augmented agents\\' capability to autoformalize strategies for gameplay.</p>\"}',\n",
       " b'{\"title\": \"Kajal: Extracting Grammar of a Source Code Using Large Language Models\", \"link\": \"https://papers.cool/arxiv/2412.08842\", \"description\": \"<a href=\\\\\"https://arxiv.org/pdf/2412.08842.pdf\\\\\">[PDF]</a>   <a href=\\\\\"https://papers.cool/arxiv/2412.08842\\\\\">[Site]</a>   <a href=\\\\\"https://papers.cool/arxiv/2412.08842\\\\\">[Kimi]</a>   <p><b>Authors:</b> Mohammad Jalili Torkamani</p>   <p>Understanding and extracting the grammar of a domain-specific language (DSL) is crucial for various software engineering tasks; however, manually creating these grammars is time-intensive and error-prone. This paper presents Kajal, a novel approach that automatically infers grammar from DSL code snippets by leveraging Large Language Models (LLMs) through prompt engineering and few-shot learning. Kajal dynamically constructs input prompts, using contextual information to guide the LLM in generating the corresponding grammars, which are iteratively refined through a feedback-driven approach. Our experiments show that Kajal achieves 60% accuracy with few-shot learning and 45% without it, demonstrating the significant impact of few-shot learning on the tool\\'s effectiveness. This approach offers a promising solution for automating DSL grammar extraction, and future work will explore using smaller, open-source LLMs and testing on larger datasets to further validate Kajal\\'s performance.</p>\"}',\n",
       " b'{\"title\": \"Exploring Large Language Models on Cross-Cultural Values in Connection with Training Methodology\", \"link\": \"https://papers.cool/arxiv/2412.08846\", \"description\": \"<a href=\\\\\"https://arxiv.org/pdf/2412.08846.pdf\\\\\">[PDF]</a>   <a href=\\\\\"https://papers.cool/arxiv/2412.08846\\\\\">[Site]</a>   <a href=\\\\\"https://papers.cool/arxiv/2412.08846\\\\\">[Kimi]</a>   <p><b>Authors:</b> Minsang Kim</p>   <p>Large language models (LLMs) closely interact with humans, and thus need an intimate understanding of the cultural values of human society. In this paper, we explore how open-source LLMs make judgments on diverse categories of cultural values across countries, and its relation to training methodology such as model sizes, training corpus, alignment, etc. Our analysis shows that LLMs can judge socio-cultural norms similar to humans but less so on social systems and progress. In addition, LLMs tend to judge cultural values biased toward Western culture, which can be improved with training on the multilingual corpus. We also find that increasing model size helps a better understanding of social values, but smaller models can be enhanced by using synthetic data. Our analysis reveals valuable insights into the design methodology of LLMs in connection with their understanding of cultural values.</p>\"}',\n",
       " b'{\"title\": \"RuleArena: A Benchmark for Rule-Guided Reasoning with LLMs in Real-World Scenarios\", \"link\": \"https://papers.cool/arxiv/2412.08972\", \"description\": \"<a href=\\\\\"https://arxiv.org/pdf/2412.08972.pdf\\\\\">[PDF]</a>   <a href=\\\\\"https://papers.cool/arxiv/2412.08972\\\\\">[Site]</a>   <a href=\\\\\"https://papers.cool/arxiv/2412.08972\\\\\">[Kimi]</a>   <p><b>Authors:</b> Ruiwen Zhou</p>   <p>This paper introduces RuleArena, a novel and challenging benchmark designed to evaluate the ability of large language models (LLMs) to follow complex, real-world rules in reasoning. Covering three practical domains -- airline baggage fees, NBA transactions, and tax regulations -- RuleArena assesses LLMs\\' proficiency in handling intricate natural language instructions that demand long-context understanding, logical reasoning, and accurate mathematical computation. Two key attributes distinguish RuleArena from traditional rule-based reasoning benchmarks: (1) it extends beyond standard first-order logic representations, and (2) it is grounded in authentic, practical scenarios, providing insights into the suitability and reliability of LLMs for real-world applications. Our findings reveal several notable limitations in LLMs: (1) they struggle to identify and apply the appropriate rules, frequently becoming confused by similar but distinct regulations, (2) they cannot consistently perform accurate mathematical computations, even when they correctly identify the relevant rules, and (3) in general, they perform poorly in the benchmark. These results highlight significant challenges in advancing LLMs\\' rule-guided reasoning capabilities in real-life applications.</p>\"}',\n",
       " b'{\"title\": \"What Makes Cryptic Crosswords Challenging for LLMs?\", \"link\": \"https://papers.cool/arxiv/2412.09012\", \"description\": \"<a href=\\\\\"https://arxiv.org/pdf/2412.09012.pdf\\\\\">[PDF]</a>   <a href=\\\\\"https://papers.cool/arxiv/2412.09012\\\\\">[Site]</a>   <a href=\\\\\"https://papers.cool/arxiv/2412.09012\\\\\">[Kimi]</a>   <p><b>Authors:</b> Abdelrahman Sadallah</p>   <p>Cryptic crosswords are puzzles that rely on general knowledge and the solver\\'s ability to manipulate language on different levels, dealing with various types of wordplay. Previous research suggests that solving such puzzles is challenging even for modern NLP models, including Large Language Models (LLMs). However, there is little to no research on the reasons for their poor performance on this task. In this paper, we establish the benchmark results for three popular LLMs: Gemma2, LLaMA3 and ChatGPT, showing that their performance on this task is still significantly below that of humans. We also investigate why these models struggle to achieve superior performance. We release our code and introduced datasets at https://github.com/bodasadallah/decrypting-crosswords.</p>\"}',\n",
       " b'{\"title\": \"Multi-Task Learning with LLMs for Implicit Sentiment Analysis: Data-level and Task-level Automatic Weight Learning\", \"link\": \"https://papers.cool/arxiv/2412.09046\", \"description\": \"<a href=\\\\\"https://arxiv.org/pdf/2412.09046.pdf\\\\\">[PDF]</a>   <a href=\\\\\"https://papers.cool/arxiv/2412.09046\\\\\">[Site]</a>   <a href=\\\\\"https://papers.cool/arxiv/2412.09046\\\\\">[Kimi]</a>   <p><b>Authors:</b> Wenna Lai</p>   <p>Implicit sentiment analysis (ISA) presents significant challenges due to the absence of salient cue words. Previous methods have struggled with insufficient data and limited reasoning capabilities to infer underlying opinions. Integrating multi-task learning (MTL) with large language models (LLMs) offers the potential to enable models of varying sizes to reliably perceive and recognize genuine opinions in ISA. However, existing MTL approaches are constrained by two sources of uncertainty: data-level uncertainty, arising from hallucination problems in LLM-generated contextual information, and task-level uncertainty, stemming from the varying capacities of models to process contextual information. To handle these uncertainties, we introduce MT-ISA, a novel MTL framework that enhances ISA by leveraging the generation and reasoning capabilities of LLMs through automatic MTL. Specifically, MT-ISA constructs auxiliary tasks using generative LLMs to supplement sentiment elements and incorporates automatic MTL to fully exploit auxiliary data. We introduce data-level and task-level automatic weight learning (AWL), which dynamically identifies relationships and prioritizes more reliable data and critical tasks, enabling models of varying sizes to adaptively learn fine-grained weights based on their reasoning capabilities. We investigate three strategies for data-level AWL, while also introducing homoscedastic uncertainty for task-level AWL. Extensive experiments reveal that models of varying sizes achieve an optimal balance between primary prediction and auxiliary tasks in MT-ISA. This underscores the effectiveness and adaptability of our approach.</p>\"}',\n",
       " b'{\"title\": \"EmbedGenius: Towards Automated Software Development for Generic Embedded IoT Systems\", \"link\": \"https://papers.cool/arxiv/2412.09058\", \"description\": \"<a href=\\\\\"https://arxiv.org/pdf/2412.09058.pdf\\\\\">[PDF]</a>   <a href=\\\\\"https://papers.cool/arxiv/2412.09058\\\\\">[Site]</a>   <a href=\\\\\"https://papers.cool/arxiv/2412.09058\\\\\">[Kimi]</a>   <p><b>Authors:</b> Huanqi Yang</p>   <p>Embedded IoT system development is crucial for enabling seamless connectivity and functionality across a wide range of applications. However, such a complex process requires cross-domain knowledge of hardware and software and hence often necessitates direct developer involvement, making it labor-intensive, time-consuming, and error-prone. To address this challenge, this paper introduces EmbedGenius, the first fully automated software development platform for general-purpose embedded IoT systems. The key idea is to leverage the reasoning ability of Large Language Models (LLMs) and embedded system expertise to automate the hardware-in-the-loop development process. The main methods include a component-aware library resolution method for addressing hardware dependencies, a library knowledge generation method that injects utility domain knowledge into LLMs, and an auto-programming method that ensures successful deployment. We evaluate EmbedGenius\\'s performance across 71 modules and four mainstream embedded development platforms with over 350 IoT tasks. Experimental results show that EmbedGenius can generate codes with an accuracy of 95.7% and complete tasks with a success rate of 86.5%, surpassing human-in-the-loop baselines by 15.6%--37.7% and 25.5%--53.4%, respectively. We also show EmbedGenius\\'s potential through case studies in environmental monitoring and remote control systems development.</p>\"}',\n",
       " b'{\"title\": \"Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning\", \"link\": \"https://papers.cool/arxiv/2412.09078\", \"description\": \"<a href=\\\\\"https://arxiv.org/pdf/2412.09078.pdf\\\\\">[PDF]</a>   <a href=\\\\\"https://papers.cool/arxiv/2412.09078\\\\\">[Site]</a>   <a href=\\\\\"https://papers.cool/arxiv/2412.09078\\\\\">[Kimi]</a>   <p><b>Authors:</b> Zhenni Bi</p>   <p>Large Language Models (LLMs) have shown remarkable abilities across various language tasks, but solving complex reasoning problems remains a challenge. While existing methods like Chain-of-Thought (CoT) and Tree-of-Thought (ToT) enhance reasoning by decomposing problems or structuring prompts, they typically perform a single pass of reasoning and may fail to revisit flawed paths, compromising accuracy. To address this, we propose a novel reasoning framework called Forest-of-Thought (FoT), which integrates multiple reasoning trees to leverage collective decision-making for solving complex logical problems. FoT utilizes sparse activation strategies to select the most relevant reasoning paths, improving both efficiency and accuracy. Additionally, we introduce a dynamic self-correction strategy that enables real-time error correction and learning from past mistakes, as well as consensus-guided decision making strategies to optimize correctness and computational resources. Experimental results demonstrate that the FoT framework, combined with these strategies, significantly enhances the reasoning capabilities of LLMs, enabling them to solve complex tasks with greater precision and efficiency.</p>\"}',\n",
       " b'{\"title\": \"Filter-then-Generate: Large Language Models with Structure-Text Adapter for Knowledge Graph Completion\", \"link\": \"https://papers.cool/arxiv/2412.09094\", \"description\": \"<a href=\\\\\"https://arxiv.org/pdf/2412.09094.pdf\\\\\">[PDF]</a>   <a href=\\\\\"https://papers.cool/arxiv/2412.09094\\\\\">[Site]</a>   <a href=\\\\\"https://papers.cool/arxiv/2412.09094\\\\\">[Kimi]</a>   <p><b>Authors:</b> Ben Liu</p>   <p>Large Language Models (LLMs) present massive inherent knowledge and superior semantic comprehension capability, which have revolutionized various tasks in natural language processing. Despite their success, a critical gap remains in enabling LLMs to perform knowledge graph completion (KGC). Empirical evidence suggests that LLMs consistently perform worse than conventional KGC approaches, even through sophisticated prompt design or tailored instruction-tuning. Fundamentally, applying LLMs on KGC introduces several critical challenges, including a vast set of entity candidates, hallucination issue of LLMs, and under-exploitation of the graph structure. To address these challenges, we propose a novel instruction-tuning-based method, namely FtG. Specifically, we present a \\\\\\\\textit{filter-then-generate} paradigm and formulate the KGC task into a multiple-choice question format. In this way, we can harness the capability of LLMs while mitigating the issue casused by hallucinations. Moreover, we devise a flexible ego-graph serialization prompt and employ a structure-text adapter to couple structure and text information in a contextualized manner. Experimental results demonstrate that FtG achieves substantial performance gain compared to existing state-of-the-art methods. The instruction dataset and code are available at \\\\\\\\url{https://github.com/LB0828/FtG}.</p>\"}',\n",
       " b'{\"title\": \"When Text Embedding Meets Large Language Model: A Comprehensive Survey\", \"link\": \"https://papers.cool/arxiv/2412.09165\", \"description\": \"<a href=\\\\\"https://arxiv.org/pdf/2412.09165.pdf\\\\\">[PDF]</a>   <a href=\\\\\"https://papers.cool/arxiv/2412.09165\\\\\">[Site]</a>   <a href=\\\\\"https://papers.cool/arxiv/2412.09165\\\\\">[Kimi]</a>   <p><b>Authors:</b> Zhijie Nie</p>   <p>Text embedding has become a foundational technology in natural language processing (NLP) during the deep learning era, driving advancements across a wide array of downstream tasks. While many natural language understanding challenges can now be modeled using generative paradigms and leverage the robust generative and comprehension capabilities of large language models (LLMs), numerous practical applications, such as semantic matching, clustering, and information retrieval, continue to rely on text embeddings for their efficiency and effectiveness. In this survey, we categorize the interplay between LLMs and text embeddings into three overarching themes: (1) LLM-augmented text embedding, enhancing traditional embedding methods with LLMs; (2) LLMs as text embedders, utilizing their innate capabilities for embedding generation; and (3) Text embedding understanding with LLMs, leveraging LLMs to analyze and interpret embeddings. By organizing these efforts based on interaction patterns rather than specific downstream applications, we offer a novel and systematic overview of contributions from various research and application domains in the era of LLMs. Furthermore, we highlight the unresolved challenges that persisted in the pre-LLM era with pre-trained language models (PLMs) and explore the emerging obstacles brought forth by LLMs. Building on this analysis, we outline prospective directions for the evolution of text embedding, addressing both theoretical and practical opportunities in the rapidly advancing landscape of NLP.</p>\"}',\n",
       " b'{\"title\": \"LMAgent: A Large-scale Multimodal Agents Society for Multi-user Simulation\", \"link\": \"https://papers.cool/arxiv/2412.09237\", \"description\": \"<a href=\\\\\"https://arxiv.org/pdf/2412.09237.pdf\\\\\">[PDF]</a>   <a href=\\\\\"https://papers.cool/arxiv/2412.09237\\\\\">[Site]</a>   <a href=\\\\\"https://papers.cool/arxiv/2412.09237\\\\\">[Kimi]</a>   <p><b>Authors:</b> Yijun Liu</p>   <p>The believable simulation of multi-user behavior is crucial for understanding complex social systems. Recently, large language models (LLMs)-based AI agents have made significant progress, enabling them to achieve human-like intelligence across various tasks. However, real human societies are often dynamic and complex, involving numerous individuals engaging in multimodal interactions. In this paper, taking e-commerce scenarios as an example, we present LMAgent, a very large-scale and multimodal agents society based on multimodal LLMs. In LMAgent, besides freely chatting with friends, the agents can autonomously browse, purchase, and review products, even perform live streaming e-commerce. To simulate this complex system, we introduce a self-consistency prompting mechanism to augment agents\\' multimodal capabilities, resulting in significantly improved decision-making performance over the existing multi-agent system. Moreover, we propose a fast memory mechanism combined with the small-world model to enhance system efficiency, which supports more than 10,000 agent simulations in a society. Experiments on agents\\' behavior show that these agents achieve comparable performance to humans in behavioral indicators. Furthermore, compared with the existing LLMs-based multi-agent system, more different and valuable phenomena are exhibited, such as herd behavior, which demonstrates the potential of LMAgent in credible large-scale social behavior simulations.</p>\"}',\n",
       " b'{\"title\": \"Towards Understanding the Robustness of LLM-based Evaluations under Perturbations\", \"link\": \"https://papers.cool/arxiv/2412.09269\", \"description\": \"<a href=\\\\\"https://arxiv.org/pdf/2412.09269.pdf\\\\\">[PDF]</a>   <a href=\\\\\"https://papers.cool/arxiv/2412.09269\\\\\">[Site]</a>   <a href=\\\\\"https://papers.cool/arxiv/2412.09269\\\\\">[Kimi]</a>   <p><b>Authors:</b> Manav Chaudhary</p>   <p>Traditional evaluation metrics like BLEU and ROUGE fall short when capturing the nuanced qualities of generated text, particularly when there is no single ground truth. In this paper, we explore the potential of Large Language Models (LLMs), specifically Google Gemini 1, to serve as automatic evaluators for non-standardized metrics in summarization and dialog-based tasks. We conduct experiments across multiple prompting strategies to examine how LLMs fare as quality evaluators when compared with human judgments on the SummEval and USR datasets, asking the model to generate both a score as well as a justification for the score. Furthermore, we explore the robustness of the LLM evaluator by using perturbed inputs. Our findings suggest that while LLMs show promise, their alignment with human evaluators is limited, they are not robust against perturbations and significant improvements are required for their standalone use as reliable evaluators for subjective metrics.</p>\"}',\n",
       " b'{\"title\": \"Towards a Multimodal Large Language Model with Pixel-Level Insight for Biomedicine\", \"link\": \"https://papers.cool/arxiv/2412.09278\", \"description\": \"<a href=\\\\\"https://arxiv.org/pdf/2412.09278.pdf\\\\\">[PDF]</a>   <a href=\\\\\"https://papers.cool/arxiv/2412.09278\\\\\">[Site]</a>   <a href=\\\\\"https://papers.cool/arxiv/2412.09278\\\\\">[Kimi]</a>   <p><b>Authors:</b> Xiaoshuang Huang</p>   <p>In recent years, Multimodal Large Language Models (MLLM) have achieved notable advancements, demonstrating the feasibility of developing an intelligent biomedical assistant. However, current biomedical MLLMs predominantly focus on image-level understanding and restrict interactions to textual commands, thus limiting their capability boundaries and the flexibility of usage. In this paper, we introduce a novel end-to-end multimodal large language model for the biomedical domain, named MedPLIB, which possesses pixel-level understanding. Excitingly, it supports visual question answering (VQA), arbitrary pixel-level prompts (points, bounding boxes, and free-form shapes), and pixel-level grounding. We propose a novel Mixture-of-Experts (MoE) multi-stage training strategy, which divides MoE into separate training phases for a visual-language expert model and a pixel-grounding expert model, followed by fine-tuning using MoE. This strategy effectively coordinates multitask learning while maintaining the computational cost at inference equivalent to that of a single expert model. To advance the research of biomedical MLLMs, we introduce the Medical Complex Vision Question Answering Dataset (MeCoVQA), which comprises an array of 8 modalities for complex medical imaging question answering and image region understanding. Experimental results indicate that MedPLIB has achieved state-of-the-art outcomes across multiple medical visual language tasks. More importantly, in zero-shot evaluations for the pixel grounding task, MedPLIB leads the best small and large models by margins of 19.7 and 15.6 respectively on the mDice metric. The codes, data, and model checkpoints will be made publicly available at https://github.com/ShawnHuang497/MedPLIB.</p>\"}',\n",
       " b'{\"title\": \"Benchmarking LLMs for Mimicking Child-Caregiver Language in Interaction\", \"link\": \"https://papers.cool/arxiv/2412.09318\", \"description\": \"<a href=\\\\\"https://arxiv.org/pdf/2412.09318.pdf\\\\\">[PDF]</a>   <a href=\\\\\"https://papers.cool/arxiv/2412.09318\\\\\">[Site]</a>   <a href=\\\\\"https://papers.cool/arxiv/2412.09318\\\\\">[Kimi]</a>   <p><b>Authors:</b> Jing Liu</p>   <p>LLMs can generate human-like dialogues, yet their ability to simulate early child-adult interactions remains largely unexplored. In this paper, we examined how effectively LLMs can capture the distinctive features of child-caregiver language in interaction, using both static and interactive benchmarking methods. We found that state-of-the-art LLMs like Llama 3 and GPT-4o can approximate child-caregiver dialogues at the word and utterance level, but they struggle to reproduce the child and caregiver\\'s discursive patterns, exaggerate alignment, and fail to reach the level of diversity shown by humans. The broader goal of this work is to initiate the development of a comprehensive benchmark for LLMs in child-oriented applications.</p>\"}',\n",
       " b'{\"title\": \"AI Predicts AGI: Leveraging AGI Forecasting and Peer Review to Explore LLMs\\' Complex Reasoning Capabilities\", \"link\": \"https://papers.cool/arxiv/2412.09385\", \"description\": \"<a href=\\\\\"https://arxiv.org/pdf/2412.09385.pdf\\\\\">[PDF]</a>   <a href=\\\\\"https://papers.cool/arxiv/2412.09385\\\\\">[Site]</a>   <a href=\\\\\"https://papers.cool/arxiv/2412.09385\\\\\">[Kimi]</a>   <p><b>Authors:</b> Fabrizio Davide</p>   <p>We tasked 16 state-of-the-art large language models (LLMs) with estimating the likelihood of Artificial General Intelligence (AGI) emerging by 2030. To assess the quality of these forecasts, we implemented an automated peer review process (LLM-PR). The LLMs\\' estimates varied widely, ranging from 3% (Reka- Core) to 47.6% (GPT-4o), with a median of 12.5%. These estimates closely align with a recent expert survey that projected a 10% likelihood of AGI by 2027, underscoring the relevance of LLMs in forecasting complex, speculative scenarios. The LLM-PR process demonstrated strong reliability, evidenced by a high Intraclass Correlation Coefficient (ICC = 0.79), reflecting notable consistency in scoring across the models. Among the models, Pplx-70b-online emerged as the top performer, while Gemini-1.5-pro-api ranked the lowest. A cross-comparison with external benchmarks, such as LMSYS Chatbot Arena, revealed that LLM rankings remained consistent across different evaluation methods, suggesting that existing benchmarks may not encapsulate some of the skills relevant for AGI prediction. We further explored the use of weighting schemes based on external benchmarks, optimizing the alignment of LLMs\\' predictions with human expert forecasts. This analysis led to the development of a new, \\'AGI benchmark\\' designed to highlight performance differences in AGI-related tasks. Our findings offer insights into LLMs\\' capabilities in speculative, interdisciplinary forecasting tasks and emphasize the growing need for innovative evaluation frameworks for assessing AI performance in complex, uncertain real-world scenarios.</p>\"}',\n",
       " b'{\"title\": \"From Intention To Implementation: Automating Biomedical Research via LLMs\", \"link\": \"https://papers.cool/arxiv/2412.09429\", \"description\": \"<a href=\\\\\"https://arxiv.org/pdf/2412.09429.pdf\\\\\">[PDF]</a>   <a href=\\\\\"https://papers.cool/arxiv/2412.09429\\\\\">[Site]</a>   <a href=\\\\\"https://papers.cool/arxiv/2412.09429\\\\\">[Kimi]</a>   <p><b>Authors:</b> Yi Luo</p>   <p>Conventional biomedical research is increasingly labor-intensive due to the exponential growth of scientific literature and datasets. Artificial intelligence (AI), particularly Large Language Models (LLMs), has the potential to revolutionize this process by automating various steps. Still, significant challenges remain, including the need for multidisciplinary expertise, logicality of experimental design, and performance measurements. This paper introduces BioResearcher, the first end-to-end automated system designed to streamline the entire biomedical research process involving dry lab experiments. BioResearcher employs a modular multi-agent architecture, integrating specialized agents for search, literature processing, experimental design, and programming. By decomposing complex tasks into logically related sub-tasks and utilizing a hierarchical learning approach, BioResearcher effectively addresses the challenges of multidisciplinary requirements and logical complexity. Furthermore, BioResearcher incorporates an LLM-based reviewer for in-process quality control and introduces novel evaluation metrics to assess the quality and automation of experimental protocols. BioResearcher successfully achieves an average execution success rate of 63.07% across eight previously unmet research objectives. The generated protocols averagely outperform typical agent systems by 22.0% on five quality metrics. The system demonstrates significant potential to reduce researchers\\' workloads and accelerate biomedical discoveries, paving the way for future innovations in automated research systems.</p>\"}',\n",
       " b'{\"title\": \"JuStRank: Benchmarking LLM Judges for System Ranking\", \"link\": \"https://papers.cool/arxiv/2412.09569\", \"description\": \"<a href=\\\\\"https://arxiv.org/pdf/2412.09569.pdf\\\\\">[PDF]</a>   <a href=\\\\\"https://papers.cool/arxiv/2412.09569\\\\\">[Site]</a>   <a href=\\\\\"https://papers.cool/arxiv/2412.09569\\\\\">[Kimi]</a>   <p><b>Authors:</b> Ariel Gera</p>   <p>Given the rapid progress of generative AI, there is a pressing need to systematically compare and choose between the numerous models and configurations available. The scale and versatility of such evaluations make the use of LLM-based judges a compelling solution for this challenge. Crucially, this approach requires first to validate the quality of the LLM judge itself. Previous work has focused on instance-based assessment of LLM judges, where a judge is evaluated over a set of responses, or response pairs, while being agnostic to their source systems. We argue that this setting overlooks critical factors affecting system-level ranking, such as a judge\\'s positive or negative bias towards certain systems. To address this gap, we conduct the first large-scale study of LLM judges as system rankers. System scores are generated by aggregating judgment scores over multiple system outputs, and the judge\\'s quality is assessed by comparing the resulting system ranking to a human-based ranking. Beyond overall judge assessment, our analysis provides a fine-grained characterization of judge behavior, including their decisiveness and bias.</p>\"}',\n",
       " b'{\"title\": \"Neptune: The Long Orbit to Benchmarking Long Video Understanding\", \"link\": \"https://papers.cool/arxiv/2412.09582\", \"description\": \"<a href=\\\\\"https://arxiv.org/pdf/2412.09582.pdf\\\\\">[PDF]</a>   <a href=\\\\\"https://papers.cool/arxiv/2412.09582\\\\\">[Site]</a>   <a href=\\\\\"https://papers.cool/arxiv/2412.09582\\\\\">[Kimi]</a>   <p><b>Authors:</b> Arsha Nagrani</p>   <p>This paper describes a semi-automatic pipeline to generate challenging question-answer-decoy sets for understanding long videos. Many existing video datasets and models are focused on short clips (10s-30s). While some long video datasets do exist, they can often be solved by powerful image models applied per frame (and often to very few frames) in a video, and are usually manually annotated at high cost. In order to mitigate both these problems, we propose a scalable dataset creation pipeline which leverages large models (VLMs and LLMs), to automatically generate dense, time-aligned video captions, as well as tough question answer decoy sets for video segments (up to 15 minutes in length). Our dataset Neptune covers a broad range of long video reasoning abilities and consists of a subset that emphasizes multimodal reasoning. Since existing metrics for open-ended question answering are either rule-based or may rely on proprietary models, we provide a new open source model-based metric GEM to score open-ended responses on Neptune. Benchmark evaluations reveal that most current open-source long video models perform poorly on Neptune, particularly on questions testing temporal ordering, counting and state changes. Through Neptune, we aim to spur the development of more advanced models capable of understanding long videos. The dataset is available at https://github.com/google-deepmind/neptune</p>\"}',\n",
       " b'{\"title\": \"InternLM-XComposer2.5-OmniLive: A Comprehensive Multimodal System for Long-term Streaming Video and Audio Interactions\", \"link\": \"https://papers.cool/arxiv/2412.09596\", \"description\": \"<a href=\\\\\"https://arxiv.org/pdf/2412.09596.pdf\\\\\">[PDF]</a>   <a href=\\\\\"https://papers.cool/arxiv/2412.09596\\\\\">[Site]</a>   <a href=\\\\\"https://papers.cool/arxiv/2412.09596\\\\\">[Kimi]</a>   <p><b>Authors:</b> Pan Zhang</p>   <p>Creating AI systems that can interact with environments over long periods, similar to human cognition, has been a longstanding research goal. Recent advancements in multimodal large language models (MLLMs) have made significant strides in open-world understanding. However, the challenge of continuous and simultaneous streaming perception, memory, and reasoning remains largely unexplored. Current MLLMs are constrained by their sequence-to-sequence architecture, which limits their ability to process inputs and generate responses simultaneously, akin to being unable to think while perceiving. Furthermore, relying on long contexts to store historical data is impractical for long-term interactions, as retaining all information becomes costly and inefficient. Therefore, rather than relying on a single foundation model to perform all functions, this project draws inspiration from the concept of the Specialized Generalist AI and introduces disentangled streaming perception, reasoning, and memory mechanisms, enabling real-time interaction with streaming video and audio input. The proposed framework InternLM-XComposer2.5-OmniLive (IXC2.5-OL) consists of three key modules: (1) Streaming Perception Module: Processes multimodal information in real-time, storing key details in memory and triggering reasoning in response to user queries. (2) Multi-modal Long Memory Module: Integrates short-term and long-term memory, compressing short-term memories into long-term ones for efficient retrieval and improved accuracy. (3) Reasoning Module: Responds to queries and executes reasoning tasks, coordinating with the perception and memory modules. This project simulates human-like cognition, enabling multimodal large language models to provide continuous and adaptive service over time.</p>\"}',\n",
       " b'{\"title\": \"TimeRefine: Temporal Grounding with Time Refining Video LLM\", \"link\": \"https://papers.cool/arxiv/2412.09601\", \"description\": \"<a href=\\\\\"https://arxiv.org/pdf/2412.09601.pdf\\\\\">[PDF]</a>   <a href=\\\\\"https://papers.cool/arxiv/2412.09601\\\\\">[Site]</a>   <a href=\\\\\"https://papers.cool/arxiv/2412.09601\\\\\">[Kimi]</a>   <p><b>Authors:</b> Xizi Wang</p>   <p>Video temporal grounding aims to localize relevant temporal boundaries in a video given a textual prompt. Recent work has focused on enabling Video LLMs to perform video temporal grounding via next-token prediction of temporal timestamps. However, accurately localizing timestamps in videos remains challenging for Video LLMs when relying solely on temporal token prediction. Our proposed TimeRefine addresses this challenge in two ways. First, instead of directly predicting the start and end timestamps, we reformulate the temporal grounding task as a temporal refining task: the model first makes rough predictions and then refines them by predicting offsets to the target segment. This refining process is repeated multiple times, through which the model progressively self-improves its temporal localization accuracy. Second, to enhance the model\\'s temporal perception capabilities, we incorporate an auxiliary prediction head that penalizes the model more if a predicted segment deviates further from the ground truth, thus encouraging the model to make closer and more accurate predictions. Our plug-and-play method can be integrated into most LLM-based temporal grounding approaches. The experimental results demonstrate that TimeRefine achieves 3.6% and 5.0% mIoU improvements on the ActivityNet and Charades-STA datasets, respectively. Code and pretrained models will be released.</p>\"}',\n",
       " b'{\"title\": \"Olympus: A Universal Task Router for Computer Vision Tasks\", \"link\": \"https://papers.cool/arxiv/2412.09612\", \"description\": \"<a href=\\\\\"https://arxiv.org/pdf/2412.09612.pdf\\\\\">[PDF]</a>   <a href=\\\\\"https://papers.cool/arxiv/2412.09612\\\\\">[Site]</a>   <a href=\\\\\"https://papers.cool/arxiv/2412.09612\\\\\">[Kimi]</a>   <p><b>Authors:</b> Yuanze Lin</p>   <p>We introduce Olympus, a new approach that transforms Multimodal Large Language Models (MLLMs) into a unified framework capable of handling a wide array of computer vision tasks. Utilizing a controller MLLM, Olympus delegates over 20 specialized tasks across images, videos, and 3D objects to dedicated modules. This instruction-based routing enables complex workflows through chained actions without the need for training heavy generative models. Olympus easily integrates with existing MLLMs, expanding their capabilities with comparable performance. Experimental results demonstrate that Olympus achieves an average routing accuracy of 94.75% across 20 tasks and precision of 91.82% in chained action scenarios, showcasing its effectiveness as a universal task router that can solve a diverse range of computer vision tasks. Project page: https://github.com/yuanze-lin/Olympus_page</p>\"}']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# entries\n",
    "\n",
    "redis_client.lrange('filtered_rss', 0, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'1']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redis_client.lrem(\"A\", 0, \"1\")\n",
    "redis_client.lpush(\"A\", \"1\")\n",
    "\n",
    "redis_client.lrange('A', 0, -1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
